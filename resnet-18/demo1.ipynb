{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-entropy + L2 Regularization Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 593242\n",
      "Validation set size: 104690\n",
      "Test set size: 116323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 1159/1159 [16:53<00:00,  1.14it/s, accuracy=0.848, loss=4.08]\n",
      "Validating: 100%|██████████| 205/205 [01:53<00:00,  1.81it/s, accuracy=0.795, val_loss=3.84]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 4.0807, Val Loss: 3.8422, Train Acc: 0.8478, Val Acc: 0.7947, Train Prec: 0.8292, Val Prec: 0.8025, Train F1: 0.8312, Val F1: 0.7627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 1159/1159 [19:18<00:00,  1.00it/s, accuracy=0.859, loss=3.44]\n",
      "Validating: 100%|██████████| 205/205 [02:52<00:00,  1.19it/s, accuracy=0.814, val_loss=3.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Train Loss: 3.4376, Val Loss: 3.5269, Train Acc: 0.8592, Val Acc: 0.8139, Train Prec: 0.8452, Val Prec: 0.8298, Train F1: 0.8418, Val F1: 0.7802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 1159/1159 [24:45<00:00,  1.28s/it, accuracy=0.862, loss=3.31] \n",
      "Validating: 100%|██████████| 205/205 [02:21<00:00,  1.44it/s, accuracy=0.841, val_loss=3.37]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Train Loss: 3.3120, Val Loss: 3.3737, Train Acc: 0.8615, Val Acc: 0.8409, Train Prec: 0.8476, Val Prec: 0.8382, Train F1: 0.8443, Val F1: 0.8218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 1159/1159 [21:08<00:00,  1.09s/it, accuracy=0.862, loss=3.27]\n",
      "Validating: 100%|██████████| 205/205 [02:32<00:00,  1.34it/s, accuracy=0.82, val_loss=3.4]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Train Loss: 3.2659, Val Loss: 3.4024, Train Acc: 0.8625, Val Acc: 0.8203, Train Prec: 0.8538, Val Prec: 0.8193, Train F1: 0.8454, Val F1: 0.7992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 1159/1159 [21:55<00:00,  1.13s/it, accuracy=0.862, loss=3.25]\n",
      "Validating: 100%|██████████| 205/205 [02:24<00:00,  1.42it/s, accuracy=0.8, val_loss=3.49]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Train Loss: 3.2481, Val Loss: 3.4858, Train Acc: 0.8621, Val Acc: 0.7997, Train Prec: 0.8490, Val Prec: 0.8079, Train F1: 0.8453, Val F1: 0.7757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:   1%|          | 8/1159 [00:08<21:25,  1.12s/it, accuracy=0.862, loss=3.24]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 245\u001b[0m\n\u001b[1;32m    242\u001b[0m classifier \u001b[38;5;241m=\u001b[39m ImageClassifier(model, optimizer, criterion)\n\u001b[1;32m    244\u001b[0m \u001b[38;5;66;03m# Train the classifier\u001b[39;00m\n\u001b[0;32m--> 245\u001b[0m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# Test the classifier\u001b[39;00m\n\u001b[1;32m    248\u001b[0m classifier\u001b[38;5;241m.\u001b[39mtest(test_loader)\n",
      "Cell \u001b[0;32mIn[1], line 81\u001b[0m, in \u001b[0;36mImageClassifier.train\u001b[0;34m(self, train_loader, val_loader, n_epochs, patience)\u001b[0m\n\u001b[1;32m     78\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 81\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Gather predictions and true labels for accuracy/metrics calculation\u001b[39;00m\n\u001b[1;32m     84\u001b[0m preds \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet18_Weights\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import EMNIST\n",
    "from torch.utils.data import random_split\n",
    "from sklearn.metrics import precision_score, f1_score\n",
    "\n",
    "class ImageClassifier:\n",
    "    def __init__(self, network, optimizer, criterion, l2_lambda=0.01):\n",
    "        self.network = network\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.l2_lambda = l2_lambda\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.network.to(self.device)\n",
    "    \n",
    "    def _regularize(self, network, l2_lambda):\n",
    "        # Compute L2 regularization\n",
    "        l2_reg = 0.0\n",
    "        for param in network.parameters():\n",
    "            l2_reg += torch.norm(param, 2)\n",
    "        return l2_lambda * l2_reg\n",
    "            \n",
    "    def compute_loss(self, outputs, targets, l2_lambda=0.01):\n",
    "        # Compute the cross-entropy loss\n",
    "        ce_loss = self.criterion(outputs, targets)\n",
    "        \n",
    "        # Compute regularization loss\n",
    "        l2_reg = self._regularize(self.network, l2_lambda)\n",
    "        \n",
    "        return ce_loss + l2_reg\n",
    "    \n",
    "    def compute_metrics(self, preds, targets):\n",
    "        \"\"\"Helper function to compute accuracy, precision, and F1 score.\"\"\"\n",
    "        # Ensure preds are already in label form (if not already converted)\n",
    "        if preds.dim() > 1:  # Check if preds need reduction\n",
    "            preds = preds.argmax(dim=1)  # Get the predicted labels\n",
    "        \n",
    "        preds = preds.cpu().numpy()  # Convert predictions to NumPy\n",
    "        targets = targets.cpu().numpy()  # Convert true labels to NumPy\n",
    "\n",
    "        # Compute accuracy\n",
    "        accuracy = (preds == targets).mean()\n",
    "\n",
    "        # Compute precision and F1 score using scikit-learn\n",
    "        precision = precision_score(targets, preds, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(targets, preds, average='weighted')\n",
    "\n",
    "        return accuracy, precision, f1\n",
    "\n",
    "    def train(self, train_loader, val_loader, n_epochs=10, patience=3):\n",
    "        best_val_loss = float('inf')\n",
    "        current_patience = 0\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            # Train\n",
    "            self.network.train()\n",
    "            train_loss = 0.0\n",
    "            all_preds = []\n",
    "            all_targets = []\n",
    "            \n",
    "            # Use tqdm for progress bar and set dynamic description\n",
    "            train_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Training Epoch {epoch + 1}')\n",
    "            for batch_idx, (data, target) in train_bar:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.network(data)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = self.compute_loss(outputs, target)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "\n",
    "                # Gather predictions and true labels for accuracy/metrics calculation\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                all_preds.append(preds)\n",
    "                all_targets.append(target)\n",
    "                \n",
    "                # Update progress bar with loss and accuracy\n",
    "                current_accuracy, _, _ = self.compute_metrics(torch.cat(all_preds), torch.cat(all_targets))\n",
    "                train_bar.set_postfix(loss=train_loss / (batch_idx + 1), accuracy=current_accuracy)\n",
    "\n",
    "            # Calculate final metrics for training\n",
    "            all_preds = torch.cat(all_preds)\n",
    "            all_targets = torch.cat(all_targets)\n",
    "            train_accuracy, train_precision, train_f1 = self.compute_metrics(all_preds, all_targets)\n",
    "            \n",
    "            # Validate\n",
    "            self.network.eval()\n",
    "            val_loss = 0.0\n",
    "            val_preds = []\n",
    "            val_targets = []\n",
    "            \n",
    "            # Use tqdm for validation progress bar\n",
    "            val_bar = tqdm(val_loader, desc='Validating')\n",
    "            with torch.no_grad():\n",
    "                for data, target in val_bar:\n",
    "                    data, target = data.to(self.device), target.to(self.device)\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    outputs = self.network(data)\n",
    "                    \n",
    "                    # Compute loss\n",
    "                    loss = self.compute_loss(outputs, target)\n",
    "                    val_loss += loss.item()\n",
    "                    \n",
    "                    # Gather predictions and true labels\n",
    "                    preds = outputs.argmax(dim=1)\n",
    "                    val_preds.append(preds)\n",
    "                    val_targets.append(target)\n",
    "\n",
    "                    # Update progress bar with validation loss and accuracy\n",
    "                    val_accuracy, _, _ = self.compute_metrics(torch.cat(val_preds), torch.cat(val_targets))\n",
    "                    val_bar.set_postfix(val_loss=val_loss / len(val_loader), accuracy=val_accuracy)\n",
    "\n",
    "            # Calculate final validation metrics\n",
    "            val_preds = torch.cat(val_preds)\n",
    "            val_targets = torch.cat(val_targets)\n",
    "            val_accuracy, val_precision, val_f1 = self.compute_metrics(val_preds, val_targets)\n",
    "\n",
    "            # Print epoch statistics\n",
    "            train_loss /= len(train_loader)\n",
    "            val_loss /= len(val_loader)\n",
    "            print(f'Epoch {epoch + 1}/{n_epochs}, '\n",
    "                  f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, '\n",
    "                  f'Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}, '\n",
    "                  f'Train Prec: {train_precision:.4f}, Val Prec: {val_precision:.4f}, '\n",
    "                  f'Train F1: {train_f1:.4f}, Val F1: {val_f1:.4f}')\n",
    "            \n",
    "            # Check for early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                current_patience = 0\n",
    "            else:\n",
    "                current_patience += 1\n",
    "                if current_patience >= patience:\n",
    "                    print(f'Validation loss did not improve for {patience} epochs. Stopping training.')\n",
    "                    break\n",
    "    \n",
    "    def test(self, test_loader):\n",
    "        self.network.eval()\n",
    "        test_loss = 0.0\n",
    "        correct = 0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        # Use tqdm for test progress bar\n",
    "        test_bar = tqdm(test_loader, desc='Testing')\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_bar:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.network(data)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = self.compute_loss(outputs, target)\n",
    "                test_loss += loss.item()\n",
    "                \n",
    "                # Gather predictions and true labels for accuracy/metrics calculation\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                all_preds.append(preds)\n",
    "                all_targets.append(target)\n",
    "                \n",
    "                # Update progress bar with test loss and accuracy\n",
    "                accuracy, _, _ = self.compute_metrics(torch.cat(all_preds), torch.cat(all_targets))\n",
    "                test_bar.set_postfix(loss=test_loss / len(test_loader), accuracy=accuracy)\n",
    "\n",
    "        # Calculate final test metrics\n",
    "        all_preds = torch.cat(all_preds)\n",
    "        all_targets = torch.cat(all_targets)\n",
    "        accuracy, precision, f1 = self.compute_metrics(all_preds, all_targets)\n",
    "\n",
    "        test_loss /= len(test_loader)\n",
    "        print(f'Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%, Precision: {precision:.2f}, F1 Score: {f1:.2f}')\n",
    "        \n",
    "# Define transformation for the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to 224x224\n",
    "    transforms.ToTensor(),            # Convert to tensor (1 channel)\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # Convert 1 channel to 3 channels (RGB)\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize for RGB\n",
    "])\n",
    "\n",
    "# Download the EMNIST ByClass dataset\n",
    "emnist_dataset = EMNIST(root='data', split='byclass', train=True, download=True, transform=transform)\n",
    "test_dataset = EMNIST(root='data', split='byclass', train=False, download=True, transform=transform)\n",
    "\n",
    "# Define the sizes for the training and validation sets\n",
    "train_size = int(0.85 * len(emnist_dataset))  # 80% for training\n",
    "val_size = len(emnist_dataset) - train_size   # remaining 20% for validation\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_dataset, val_dataset = random_split(emnist_dataset, [train_size, val_size])\n",
    "\n",
    "print(f'Training set size: {len(train_dataset)}')\n",
    "print(f'Validation set size: {len(val_dataset)}')\n",
    "print(f'Test set size: {len(test_dataset)}')\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=512)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512)\n",
    "\n",
    "# Example neural network architecture using ResNet-18\n",
    "class ResNet18Classifier(nn.Module):\n",
    "    def __init__(self, num_classes=62):\n",
    "        super(ResNet18Classifier, self).__init__()\n",
    "        self.resnet = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        \n",
    "        for name, child in self.resnet.named_children():\n",
    "            if name in ['layer1', 'layer2', 'layer3']:\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False\n",
    "            \n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Initialize the neural network, optimizer, and criterion\n",
    "model = ResNet18Classifier(num_classes=62)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create an instance of ImageClassifier\n",
    "classifier = ImageClassifier(model, optimizer, criterion)\n",
    "\n",
    "# Train the classifier\n",
    "classifier.train(train_loader, val_loader, n_epochs=10, patience=3)\n",
    "\n",
    "# Test the classifier\n",
    "classifier.test(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 228/228 [02:36<00:00,  1.45it/s, accuracy=0.84, loss=3.33] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 3.3250, Accuracy: 0.84%, Precision: 0.84, F1 Score: 0.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "classifier.test(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model after training\n",
    "torch.save(classifier.network.state_dict(), 'resnet18_classifier_setting1.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
