{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "id": "ky-x58HJFVGz",
        "outputId": "701c6d4a-7d1d-4b0d-f15c-92f8cd9dff1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://biometrics.nist.gov/cs_links/EMNIST/gzip.zip to data/EMNIST/raw/gzip.zip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 561753746/561753746 [00:09<00:00, 58311998.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/EMNIST/raw/gzip.zip to data/EMNIST/raw\n",
            "Training set size: 593242\n",
            "Validation set size: 104690\n",
            "Test set size: 116323\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 71.7MB/s]\n",
            "Training Epoch 1: 100%|██████████| 1325/1325 [42:59<00:00,  1.95s/it, accuracy=0.851, loss=0.197]\n",
            "Validating: 100%|██████████| 234/234 [04:07<00:00,  1.06s/it, accuracy=0.856, val_loss=0.173]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Train Loss: 0.1971, Val Loss: 0.1726, Train Acc: 0.8506, Val Acc: 0.8559, Train Prec: 0.8387, Val Prec: 0.8560, Train F1: 0.8405, Val F1: 0.8441\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 2: 100%|██████████| 1325/1325 [42:28<00:00,  1.92s/it, accuracy=0.869, loss=0.148]\n",
            "Validating: 100%|██████████| 234/234 [04:05<00:00,  1.05s/it, accuracy=0.843, val_loss=0.189]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/10, Train Loss: 0.1485, Val Loss: 0.1888, Train Acc: 0.8692, Val Acc: 0.8430, Train Prec: 0.8597, Val Prec: 0.8472, Train F1: 0.8594, Val F1: 0.8233\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 3: 100%|██████████| 1325/1325 [42:26<00:00,  1.92s/it, accuracy=0.875, loss=0.136]\n",
            "Validating: 100%|██████████| 234/234 [04:06<00:00,  1.05s/it, accuracy=0.865, val_loss=0.157]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/10, Train Loss: 0.1362, Val Loss: 0.1574, Train Acc: 0.8749, Val Acc: 0.8654, Train Prec: 0.8668, Val Prec: 0.8647, Train F1: 0.8654, Val F1: 0.8527\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 4: 100%|██████████| 1325/1325 [42:27<00:00,  1.92s/it, accuracy=0.879, loss=0.127]\n",
            "Validating: 100%|██████████| 234/234 [04:09<00:00,  1.07s/it, accuracy=0.857, val_loss=0.163]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/10, Train Loss: 0.1272, Val Loss: 0.1631, Train Acc: 0.8793, Val Acc: 0.8567, Train Prec: 0.8719, Val Prec: 0.8609, Train F1: 0.8702, Val F1: 0.8431\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 5:   0%|          | 3/1325 [00:05<39:14,  1.78s/it, accuracy=0.882, loss=0.136]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d273d8093867>\u001b[0m in \u001b[0;36m<cell line: 296>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;31m# Train the classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;31m# Test the classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-d273d8093867>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_loader, val_loader, n_epochs, patience)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mtrain_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'Training Epoch {epoch + 1}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_bar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.models as models\n",
        "from torchvision.models import ResNet18_Weights\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import EMNIST\n",
        "from torch.utils.data import random_split\n",
        "from sklearn.metrics import precision_score, f1_score\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def softmax_focal_loss(\n",
        "    inputs: torch.Tensor,\n",
        "    targets: torch.Tensor,\n",
        "    alpha: float = -1,\n",
        "    gamma: float = 2,\n",
        "    reduction: str = \"mean\",\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Multiclass Focal Loss with softmax.\n",
        "\n",
        "    Args:\n",
        "        inputs: A float tensor of shape (batch_size, num_classes). These are the logits (raw, unnormalized scores) for each class.\n",
        "        targets: A long tensor of shape (batch_size). Each value is the class label (0 to num_classes-1).\n",
        "        alpha: (optional) Weighting factor in range (0,1) to balance positive vs negative examples. Default = -1 (no weighting).\n",
        "        gamma: Exponent of the modulating factor (1 - p_t) to balance easy vs hard examples.\n",
        "        reduction: 'none' | 'mean' | 'sum'.\n",
        "                 'none': No reduction will be applied to the output.\n",
        "                 'mean': The output will be averaged.\n",
        "                 'sum': The output will be summed.\n",
        "\n",
        "    Returns:\n",
        "        Loss tensor with the reduction option applied.\n",
        "    \"\"\"\n",
        "    # Convert logits to probabilities using softmax\n",
        "    log_probs = F.log_softmax(inputs, dim=1)\n",
        "\n",
        "    # Gather the log_probs corresponding to the correct classes\n",
        "    targets_one_hot = F.one_hot(targets, num_classes=inputs.size(1)).float()\n",
        "    log_probs = (log_probs * targets_one_hot).sum(dim=1)\n",
        "\n",
        "    probs = torch.exp(log_probs)  # Convert log_probs back to probabilities\n",
        "\n",
        "    # Focal loss factor\n",
        "    focal_factor = (1 - probs) ** gamma\n",
        "\n",
        "    # Focal loss calculation\n",
        "    loss = -focal_factor * log_probs\n",
        "\n",
        "    if alpha >= 0:\n",
        "        alpha_t = alpha * targets_one_hot + (1 - alpha) * (1 - targets_one_hot)\n",
        "        loss = alpha_t.sum(dim=1) * loss\n",
        "\n",
        "    # Apply reduction\n",
        "    if reduction == \"mean\":\n",
        "        loss = loss.mean()\n",
        "    elif reduction == \"sum\":\n",
        "        loss = loss.sum()\n",
        "\n",
        "    return loss\n",
        "\n",
        "class ImageClassifier:\n",
        "    def __init__(self, network, optimizer, criterion, l2_lambda=0.01):\n",
        "        self.network = network\n",
        "        self.optimizer = optimizer\n",
        "        self.criterion = criterion\n",
        "        self.l2_lambda = l2_lambda\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.network.to(self.device)\n",
        "\n",
        "    def _regularize(self, network, l2_lambda):\n",
        "        # Compute L2 regularization\n",
        "        l2_reg = 0.0\n",
        "        for param in network.parameters():\n",
        "            l2_reg += torch.norm(param, 2)\n",
        "        return l2_lambda * l2_reg\n",
        "\n",
        "    def compute_loss(self, outputs, targets, l2_lambda=0.01, regularize = False):\n",
        "        # Compute the cross-entropy loss\n",
        "        ce_loss = self.criterion(outputs, targets)\n",
        "\n",
        "        if regularize:\n",
        "            # Compute regularization loss\n",
        "            l2_reg = self._regularize(self.network, l2_lambda)\n",
        "\n",
        "            return ce_loss + l2_reg\n",
        "\n",
        "        return ce_loss\n",
        "\n",
        "    def compute_metrics(self, preds, targets):\n",
        "        \"\"\"Helper function to compute accuracy, precision, and F1 score.\"\"\"\n",
        "        # Ensure preds are already in label form (if not already converted)\n",
        "        if preds.dim() > 1:  # Check if preds need reduction\n",
        "            preds = preds.argmax(dim=1)  # Get the predicted labels\n",
        "\n",
        "        preds = preds.cpu().numpy()  # Convert predictions to NumPy\n",
        "        targets = targets.cpu().numpy()  # Convert true labels to NumPy\n",
        "\n",
        "        # Compute accuracy\n",
        "        accuracy = (preds == targets).mean()\n",
        "\n",
        "        # Compute precision and F1 score using scikit-learn\n",
        "        precision = precision_score(targets, preds, average='weighted', zero_division=0)\n",
        "        f1 = f1_score(targets, preds, average='weighted')\n",
        "\n",
        "        return accuracy, precision, f1\n",
        "\n",
        "    def train(self, train_loader, val_loader, n_epochs=10, patience=3):\n",
        "        best_val_loss = float('inf')\n",
        "        current_patience = 0\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            # Train\n",
        "            self.network.train()\n",
        "            train_loss = 0.0\n",
        "            all_preds = []\n",
        "            all_targets = []\n",
        "\n",
        "            # Use tqdm for progress bar and set dynamic description\n",
        "            train_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Training Epoch {epoch + 1}')\n",
        "            for batch_idx, (data, target) in train_bar:\n",
        "                data, target = data.to(self.device), target.to(self.device)\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self.network(data)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = self.compute_loss(outputs, target)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                train_loss += loss.item()\n",
        "\n",
        "                # Gather predictions and true labels for accuracy/metrics calculation\n",
        "                preds = outputs.argmax(dim=1)\n",
        "                all_preds.append(preds)\n",
        "                all_targets.append(target)\n",
        "\n",
        "                # Update progress bar with loss and accuracy\n",
        "                current_accuracy, _, _ = self.compute_metrics(torch.cat(all_preds), torch.cat(all_targets))\n",
        "                train_bar.set_postfix(loss=train_loss / (batch_idx + 1), accuracy=current_accuracy)\n",
        "\n",
        "            # Calculate final metrics for training\n",
        "            all_preds = torch.cat(all_preds)\n",
        "            all_targets = torch.cat(all_targets)\n",
        "            train_accuracy, train_precision, train_f1 = self.compute_metrics(all_preds, all_targets)\n",
        "\n",
        "            # Validate\n",
        "            self.network.eval()\n",
        "            val_loss = 0.0\n",
        "            val_preds = []\n",
        "            val_targets = []\n",
        "\n",
        "            # Use tqdm for validation progress bar\n",
        "            val_bar = tqdm(val_loader, desc='Validating')\n",
        "            with torch.no_grad():\n",
        "                for data, target in val_bar:\n",
        "                    data, target = data.to(self.device), target.to(self.device)\n",
        "\n",
        "                    # Forward pass\n",
        "                    outputs = self.network(data)\n",
        "\n",
        "                    # Compute loss\n",
        "                    loss = self.compute_loss(outputs, target)\n",
        "                    val_loss += loss.item()\n",
        "\n",
        "                    # Gather predictions and true labels\n",
        "                    preds = outputs.argmax(dim=1)\n",
        "                    val_preds.append(preds)\n",
        "                    val_targets.append(target)\n",
        "\n",
        "                    # Update progress bar with validation loss and accuracy\n",
        "                    val_accuracy, _, _ = self.compute_metrics(torch.cat(val_preds), torch.cat(val_targets))\n",
        "                    val_bar.set_postfix(val_loss=val_loss / len(val_loader), accuracy=val_accuracy)\n",
        "\n",
        "            # Calculate final validation metrics\n",
        "            val_preds = torch.cat(val_preds)\n",
        "            val_targets = torch.cat(val_targets)\n",
        "            val_accuracy, val_precision, val_f1 = self.compute_metrics(val_preds, val_targets)\n",
        "\n",
        "            # Print epoch statistics\n",
        "            train_loss /= len(train_loader)\n",
        "            val_loss /= len(val_loader)\n",
        "            print(f'Epoch {epoch + 1}/{n_epochs}, '\n",
        "                  f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, '\n",
        "                  f'Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}, '\n",
        "                  f'Train Prec: {train_precision:.4f}, Val Prec: {val_precision:.4f}, '\n",
        "                  f'Train F1: {train_f1:.4f}, Val F1: {val_f1:.4f}')\n",
        "\n",
        "            # Check for early stopping\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                current_patience = 0\n",
        "            else:\n",
        "                current_patience += 1\n",
        "                if current_patience >= patience:\n",
        "                    print(f'Validation loss did not improve for {patience} epochs. Stopping training.')\n",
        "                    break\n",
        "\n",
        "    def test(self, test_loader):\n",
        "        self.network.eval()\n",
        "        test_loss = 0.0\n",
        "        correct = 0\n",
        "        all_preds = []\n",
        "        all_targets = []\n",
        "\n",
        "        # Use tqdm for test progress bar\n",
        "        test_bar = tqdm(test_loader, desc='Testing')\n",
        "        with torch.no_grad():\n",
        "            for data, target in test_bar:\n",
        "                data, target = data.to(self.device), target.to(self.device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self.network(data)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = self.compute_loss(outputs, target)\n",
        "                test_loss += loss.item()\n",
        "\n",
        "                # Gather predictions and true labels for accuracy/metrics calculation\n",
        "                preds = outputs.argmax(dim=1)\n",
        "                all_preds.append(preds)\n",
        "                all_targets.append(target)\n",
        "\n",
        "                # Update progress bar with test loss and accuracy\n",
        "                accuracy, _, _ = self.compute_metrics(torch.cat(all_preds), torch.cat(all_targets))\n",
        "                test_bar.set_postfix(loss=test_loss / len(test_loader), accuracy=accuracy)\n",
        "\n",
        "        # Calculate final test metrics\n",
        "        all_preds = torch.cat(all_preds)\n",
        "        all_targets = torch.cat(all_targets)\n",
        "        accuracy, precision, f1 = self.compute_metrics(all_preds, all_targets)\n",
        "\n",
        "        test_loss /= len(test_loader)\n",
        "        print(f'Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%, Precision: {precision:.2f}, F1 Score: {f1:.2f}')\n",
        "\n",
        "# Define transformation for the images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize to 224x224\n",
        "    transforms.ToTensor(),            # Convert to tensor (1 channel)\n",
        "    transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # Convert 1 channel to 3 channels (RGB)\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize for RGB\n",
        "])\n",
        "\n",
        "# Download the EMNIST ByClass dataset\n",
        "emnist_dataset = EMNIST(root='data', split='byclass', train=True, download=True, transform=transform)\n",
        "test_dataset = EMNIST(root='data', split='byclass', train=False, download=True, transform=transform)\n",
        "\n",
        "# Define the sizes for the training and validation sets\n",
        "train_size = int(0.85 * len(emnist_dataset))  # 80% for training\n",
        "val_size = len(emnist_dataset) - train_size   # remaining 20% for validation\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_dataset, val_dataset = random_split(emnist_dataset, [train_size, val_size])\n",
        "\n",
        "print(f'Training set size: {len(train_dataset)}')\n",
        "print(f'Validation set size: {len(val_dataset)}')\n",
        "print(f'Test set size: {len(test_dataset)}')\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=448, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=448)\n",
        "test_loader = DataLoader(test_dataset, batch_size=448)\n",
        "\n",
        "# Example neural network architecture using ResNet-18\n",
        "class ResNet18Classifier(nn.Module):\n",
        "    def __init__(self, num_classes=62):\n",
        "        super(ResNet18Classifier, self).__init__()\n",
        "        self.resnet = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
        "\n",
        "        for name, child in self.resnet.named_children():\n",
        "            if name in ['layer1', 'layer2', 'layer3']:\n",
        "                for param in child.parameters():\n",
        "                    param.requires_grad = False\n",
        "\n",
        "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)\n",
        "\n",
        "# Initialize the neural network, optimizer, and criterion\n",
        "model = ResNet18Classifier(num_classes=62)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = softmax_focal_loss\n",
        "\n",
        "# Create an instance of ImageClassifier\n",
        "classifier = ImageClassifier(model, optimizer, criterion)\n",
        "\n",
        "# Train the classifier\n",
        "classifier.train(train_loader, val_loader, n_epochs=10, patience=3)\n",
        "\n",
        "# Test the classifier\n",
        "classifier.test(test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YcD_pUAySGx",
        "outputId": "9f99bad7-d91a-4b7d-8a6c-c94ddff22366"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|██████████| 260/260 [04:28<00:00,  1.03s/it, accuracy=0.839, loss=0.186]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 0.1865, Accuracy: 0.84%, Precision: 0.85, F1 Score: 0.82\n"
          ]
        }
      ],
      "source": [
        "classifier.test(test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEcjuap-yHm5"
      },
      "outputs": [],
      "source": [
        "# Save the model after training\n",
        "torch.save(classifier.network.state_dict(), 'resnet18_classifier_setting3.pth')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
