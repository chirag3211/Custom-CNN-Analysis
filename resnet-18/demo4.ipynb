{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 593242\n",
      "Validation set size: 104690\n",
      "Test set size: 116323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 1159/1159 [27:19<00:00,  1.41s/it, accuracy=0.846, loss=1.31]\n",
      "Validating: 100%|██████████| 205/205 [03:35<00:00,  1.05s/it, accuracy=0.822, val_loss=0.83] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 1.3111, Val Loss: 0.8299, Train Acc: 0.8460, Val Acc: 0.8217, Train Prec: 0.8284, Val Prec: 0.8288, Train F1: 0.8309, Val F1: 0.7865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 1159/1159 [28:58<00:00,  1.50s/it, accuracy=0.859, loss=0.565]\n",
      "Validating: 100%|██████████| 205/205 [09:15<00:00,  2.71s/it, accuracy=0.739, val_loss=1.08] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Train Loss: 0.5655, Val Loss: 1.0759, Train Acc: 0.8591, Val Acc: 0.7388, Train Prec: 0.8466, Val Prec: 0.7827, Train F1: 0.8445, Val F1: 0.7190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 1159/1159 [1:04:58<00:00,  3.36s/it, accuracy=0.863, loss=0.492]\n",
      "Validating: 100%|██████████| 205/205 [07:49<00:00,  2.29s/it, accuracy=0.838, val_loss=0.561]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Train Loss: 0.4921, Val Loss: 0.5609, Train Acc: 0.8635, Val Acc: 0.8384, Train Prec: 0.8526, Val Prec: 0.8503, Train F1: 0.8497, Val F1: 0.8050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 1159/1159 [1:02:36<00:00,  3.24s/it, accuracy=0.866, loss=0.462]\n",
      "Validating: 100%|██████████| 205/205 [10:47<00:00,  3.16s/it, accuracy=0.786, val_loss=0.735]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Train Loss: 0.4618, Val Loss: 0.7350, Train Acc: 0.8663, Val Acc: 0.7860, Train Prec: 0.8557, Val Prec: 0.8074, Train F1: 0.8532, Val F1: 0.7745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5:   0%|          | 0/1159 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 317\u001b[0m\n\u001b[1;32m    314\u001b[0m classifier \u001b[38;5;241m=\u001b[39m ImageClassifier(model, optimizer, criterion, regularize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# Train the classifier\u001b[39;00m\n\u001b[0;32m--> 317\u001b[0m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreg_lambda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;66;03m# Test the classifier\u001b[39;00m\n\u001b[1;32m    320\u001b[0m classifier\u001b[38;5;241m.\u001b[39mtest(test_loader, reg_lambda\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 155\u001b[0m, in \u001b[0;36mImageClassifier.train\u001b[0;34m(self, train_loader, val_loader, n_epochs, patience, reg_lambda)\u001b[0m\n\u001b[1;32m    152\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork(data)\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreg_lambda\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[2], line 106\u001b[0m, in \u001b[0;36mImageClassifier.compute_loss\u001b[0;34m(self, outputs, targets, reg_lambda)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, outputs, targets, reg_lambda\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m):\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# Compute the cross-entropy loss\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     ce_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# Compute regularization loss\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregularize:\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;66;03m# reg = self._regularize(self.network, reg_lambda)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dev_test/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dev_test/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 25\u001b[0m, in \u001b[0;36mModifiedCrossEntropyLoss.forward\u001b[0;34m(self, inputs, targets)\u001b[0m\n\u001b[1;32m     22\u001b[0m probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(inputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Get probabilities from raw logits\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Standard cross-entropy loss for the true class\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m loss_ce \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-12\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Calculate the penalty for all classes except the true class\u001b[39;00m\n\u001b[1;32m     28\u001b[0m penalty \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpenalty_weight \u001b[38;5;241m*\u001b[39m (torch\u001b[38;5;241m.\u001b[39msum(torch\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m probs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-12\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m \n\u001b[1;32m     29\u001b[0m                                   torch\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m probs[\u001b[38;5;28mrange\u001b[39m(targets\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)), targets] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-12\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet18_Weights\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import EMNIST\n",
    "from torch.utils.data import random_split\n",
    "from sklearn.metrics import precision_score, f1_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ModifiedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, penalty_weight=0.1):\n",
    "        super(ModifiedCrossEntropyLoss, self).__init__()\n",
    "        self.penalty_weight = penalty_weight\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # Calculate probabilities using softmax\n",
    "        probs = F.softmax(inputs, dim=1)  # Get probabilities from raw logits\n",
    "\n",
    "        # Standard cross-entropy loss for the true class\n",
    "        loss_ce = torch.log(probs[range(targets.size(0)), targets] + 1e-12).mean()\n",
    "\n",
    "        # Calculate the penalty for all classes except the true class\n",
    "        penalty = self.penalty_weight * (torch.sum(torch.log(1 - probs + 1e-12), dim=1) - \n",
    "                                          torch.log(1 - probs[range(targets.size(0)), targets] + 1e-12))\n",
    "\n",
    "        # Final loss\n",
    "        total_loss = loss_ce + penalty.mean()\n",
    "        return -total_loss\n",
    "\n",
    "def conv_orth_dist(kernel, stride=1):\n",
    "    [o_c, i_c, w, h] = kernel.shape\n",
    "    assert (w == h), \"Do not support rectangular kernel\"\n",
    "\n",
    "    # Check if both stride and kernel size are 1, return zero if true\n",
    "    if stride == 1 and w == 1:\n",
    "        return torch.tensor(0.0).cuda()\n",
    "\n",
    "    assert stride < w, f\"Warning: Stride {stride} is larger than or equal to kernel size {w}.\"\n",
    "\n",
    "    new_s = stride * (w - 1) + w\n",
    "    temp = torch.eye(new_s * new_s * i_c).reshape((new_s * new_s * i_c, i_c, new_s, new_s)).cuda()\n",
    "    out = (F.conv2d(temp, kernel, stride=stride)).reshape((new_s * new_s * i_c, -1))\n",
    "\n",
    "    Vmat = out[np.floor(new_s**2 / 2).astype(int)::new_s**2, :]\n",
    "    temp = np.zeros((i_c, i_c * new_s**2))\n",
    "    for i in range(temp.shape[0]):\n",
    "        temp[i, np.floor(new_s**2 / 2).astype(int) + new_s**2 * i] = 1\n",
    "\n",
    "    return torch.norm(Vmat @ torch.t(out) - torch.from_numpy(temp).float().cuda())\n",
    "\n",
    "def deconv_orth_dist(kernel, stride=2, padding=1):\n",
    "    [o_c, i_c, w, h] = kernel.shape\n",
    "    output = F.conv_transpose2d(kernel, kernel, stride=stride, padding=padding)\n",
    "    target = torch.zeros((o_c, o_c, output.shape[-2], output.shape[-1])).cuda()\n",
    "    ct = int(np.floor(output.shape[-1] / 2))\n",
    "    target[:, :, ct, ct] = torch.eye(o_c).cuda()\n",
    "    return torch.norm(output - target)\n",
    "\n",
    "def orthogonal_regularizer(model):\n",
    "    orthogonality_loss = 0.0\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            orthogonality_loss += conv_orth_dist(layer.weight)\n",
    "        elif isinstance(layer, nn.ConvTranspose2d):\n",
    "            orthogonality_loss += deconv_orth_dist(layer.weight)\n",
    "    \n",
    "    return orthogonality_loss\n",
    "\n",
    "class ImageClassifier:\n",
    "    def __init__(self, network, optimizer, criterion, l2_lambda=0.01, regularize=False):\n",
    "        self.network = network\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.l2_lambda = l2_lambda\n",
    "        self.regularize = regularize\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.network.to(self.device)\n",
    "    \n",
    "    def _regularize(self, network, reg_lambda):\n",
    "        # Compute L2 regularization\n",
    "        l2_reg = 0.0\n",
    "        for param in network.parameters():\n",
    "            l2_reg += torch.norm(param, 2)\n",
    "        \n",
    "        return reg_lambda * l2_reg\n",
    "            \n",
    "    def orthogonalize(self, network, reg_lambda):\n",
    "        orthogonality_loss = 0.0\n",
    "        for layer in network.modules():\n",
    "            if isinstance(layer, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "                # Apply regularization only to kernels with requires_grad=True\n",
    "                if layer.weight.requires_grad:\n",
    "                    if isinstance(layer, nn.Conv2d):\n",
    "                        orthogonality_loss += conv_orth_dist(layer.weight)\n",
    "                    elif isinstance(layer, nn.ConvTranspose2d):\n",
    "                        orthogonality_loss += deconv_orth_dist(layer.weight)\n",
    "        return reg_lambda * orthogonality_loss\n",
    "\n",
    "    def compute_loss(self, outputs, targets, reg_lambda=0.01):\n",
    "        # Compute the cross-entropy loss\n",
    "        ce_loss = self.criterion(outputs, targets)\n",
    "        \n",
    "        # Compute regularization loss\n",
    "        if self.regularize:\n",
    "            # reg = self._regularize(self.network, reg_lambda)\n",
    "            reg = self.orthogonalize(self.network, reg_lambda)\n",
    "            return ce_loss + reg\n",
    "            \n",
    "        return ce_loss\n",
    "    \n",
    "    def compute_metrics(self, preds, targets):\n",
    "        \"\"\"Helper function to compute accuracy, precision, and F1 score.\"\"\"\n",
    "        # Ensure preds are already in label form (if not already converted)\n",
    "        if preds.dim() > 1:  # Check if preds need reduction\n",
    "            preds = preds.argmax(dim=1)  # Get the predicted labels\n",
    "        \n",
    "        preds = preds.cpu().numpy()  # Convert predictions to NumPy\n",
    "        targets = targets.cpu().numpy()  # Convert true labels to NumPy\n",
    "\n",
    "        # Compute accuracy\n",
    "        accuracy = (preds == targets).mean()\n",
    "\n",
    "        # Compute precision and F1 score using scikit-learn\n",
    "        precision = precision_score(targets, preds, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(targets, preds, average='weighted')\n",
    "\n",
    "        return accuracy, precision, f1\n",
    "\n",
    "    def train(self, train_loader, val_loader, n_epochs=10, patience=3, reg_lambda=0.01):\n",
    "        best_val_loss = float('inf')\n",
    "        current_patience = 0\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            # Train\n",
    "            self.network.train()\n",
    "            train_loss = 0.0\n",
    "            all_preds = []\n",
    "            all_targets = []\n",
    "            \n",
    "            # Use tqdm for progress bar and set dynamic description\n",
    "            train_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Training Epoch {epoch + 1}')\n",
    "            for batch_idx, (data, target) in train_bar:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.network(data)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = self.compute_loss(outputs, target, reg_lambda)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "\n",
    "                # Gather predictions and true labels for accuracy/metrics calculation\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                all_preds.append(preds)\n",
    "                all_targets.append(target)\n",
    "                \n",
    "                # Update progress bar with loss and accuracy\n",
    "                current_accuracy, _, _ = self.compute_metrics(torch.cat(all_preds), torch.cat(all_targets))\n",
    "                train_bar.set_postfix(loss=train_loss / (batch_idx + 1), accuracy=current_accuracy)\n",
    "\n",
    "            # Calculate final metrics for training\n",
    "            all_preds = torch.cat(all_preds)\n",
    "            all_targets = torch.cat(all_targets)\n",
    "            train_accuracy, train_precision, train_f1 = self.compute_metrics(all_preds, all_targets)\n",
    "            \n",
    "            # Validate\n",
    "            self.network.eval()\n",
    "            val_loss = 0.0\n",
    "            val_preds = []\n",
    "            val_targets = []\n",
    "            \n",
    "            # Use tqdm for validation progress bar\n",
    "            val_bar = tqdm(val_loader, desc='Validating')\n",
    "            with torch.no_grad():\n",
    "                for data, target in val_bar:\n",
    "                    data, target = data.to(self.device), target.to(self.device)\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    outputs = self.network(data)\n",
    "                    \n",
    "                    # Compute loss\n",
    "                    loss = self.compute_loss(outputs, target, reg_lambda)\n",
    "                    val_loss += loss.item()\n",
    "                    \n",
    "                    # Gather predictions and true labels\n",
    "                    preds = outputs.argmax(dim=1)\n",
    "                    val_preds.append(preds)\n",
    "                    val_targets.append(target)\n",
    "\n",
    "                    # Update progress bar with validation loss and accuracy\n",
    "                    val_accuracy, _, _ = self.compute_metrics(torch.cat(val_preds), torch.cat(val_targets))\n",
    "                    val_bar.set_postfix(val_loss=val_loss / len(val_loader), accuracy=val_accuracy)\n",
    "\n",
    "            # Calculate final validation metrics\n",
    "            val_preds = torch.cat(val_preds)\n",
    "            val_targets = torch.cat(val_targets)\n",
    "            val_accuracy, val_precision, val_f1 = self.compute_metrics(val_preds, val_targets)\n",
    "\n",
    "            # Print epoch statistics\n",
    "            train_loss /= len(train_loader)\n",
    "            val_loss /= len(val_loader)\n",
    "            print(f'Epoch {epoch + 1}/{n_epochs}, '\n",
    "                  f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, '\n",
    "                  f'Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}, '\n",
    "                  f'Train Prec: {train_precision:.4f}, Val Prec: {val_precision:.4f}, '\n",
    "                  f'Train F1: {train_f1:.4f}, Val F1: {val_f1:.4f}')\n",
    "            \n",
    "            # Check for early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                current_patience = 0\n",
    "            else:\n",
    "                current_patience += 1\n",
    "                if current_patience >= patience:\n",
    "                    print(f'Validation loss did not improve for {patience} epochs. Stopping training.')\n",
    "                    break\n",
    "    \n",
    "    def test(self, test_loader, reg_lambda=0.01):\n",
    "        self.network.eval()\n",
    "        test_loss = 0.0\n",
    "        correct = 0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        # Use tqdm for test progress bar\n",
    "        test_bar = tqdm(test_loader, desc='Testing')\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_bar:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.network(data)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = self.compute_loss(outputs, target, reg_lambda)\n",
    "                test_loss += loss.item()\n",
    "                \n",
    "                # Gather predictions and true labels for accuracy/metrics calculation\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                all_preds.append(preds)\n",
    "                all_targets.append(target)\n",
    "                \n",
    "                # Update progress bar with test loss and accuracy\n",
    "                accuracy, _, _ = self.compute_metrics(torch.cat(all_preds), torch.cat(all_targets))\n",
    "                test_bar.set_postfix(loss=test_loss / len(test_loader), accuracy=accuracy)\n",
    "\n",
    "        # Calculate final test metrics\n",
    "        all_preds = torch.cat(all_preds)\n",
    "        all_targets = torch.cat(all_targets)\n",
    "        accuracy, precision, f1 = self.compute_metrics(all_preds, all_targets)\n",
    "\n",
    "        test_loss /= len(test_loader)\n",
    "        print(f'Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%, Precision: {precision:.2f}, F1 Score: {f1:.2f}')\n",
    "        \n",
    "# Define transformation for the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to 224x224\n",
    "    transforms.ToTensor(),            # Convert to tensor (1 channel)\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # Convert 1 channel to 3 channels (RGB)\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize for RGB\n",
    "])\n",
    "\n",
    "# Download the EMNIST ByClass dataset\n",
    "emnist_dataset = EMNIST(root='data', split='byclass', train=True, download=True, transform=transform)\n",
    "test_dataset = EMNIST(root='data', split='byclass', train=False, download=True, transform=transform)\n",
    "\n",
    "# Define the sizes for the training and validation sets\n",
    "train_size = int(0.85 * len(emnist_dataset))  # 80% for training\n",
    "val_size = len(emnist_dataset) - train_size   # remaining 20% for validation\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_dataset, val_dataset = random_split(emnist_dataset, [train_size, val_size])\n",
    "\n",
    "print(f'Training set size: {len(train_dataset)}')\n",
    "print(f'Validation set size: {len(val_dataset)}')\n",
    "print(f'Test set size: {len(test_dataset)}')\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=512)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512)\n",
    "\n",
    "# Example neural network architecture using ResNet-18\n",
    "class ResNet18Classifier(nn.Module):\n",
    "    def __init__(self, num_classes=62):\n",
    "        super(ResNet18Classifier, self).__init__()\n",
    "        self.resnet = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        \n",
    "        for name, child in self.resnet.named_children():\n",
    "            if name in ['layer1', 'layer2', 'layer3']:\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False\n",
    "            \n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Initialize the neural network, optimizer, and criterion\n",
    "model = ResNet18Classifier(num_classes=62)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = ModifiedCrossEntropyLoss(penalty_weight=0.1)\n",
    "\n",
    "# Create an instance of ImageClassifier\n",
    "classifier = ImageClassifier(model, optimizer, criterion, regularize=True)\n",
    "\n",
    "# Train the classifier\n",
    "classifier.train(train_loader, val_loader, n_epochs=10, patience=3, reg_lambda=0.01)\n",
    "\n",
    "# Test the classifier\n",
    "classifier.test(test_loader, reg_lambda=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 228/228 [11:03<00:00,  2.91s/it, accuracy=0.793, loss=0.705]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.7055, Accuracy: 0.79%, Precision: 0.82, F1 Score: 0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "classifier.test(test_loader)\n",
    "\n",
    "# Save the model after training\n",
    "torch.save(classifier.network.state_dict(), 'resnet18_classifier_setting4.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
