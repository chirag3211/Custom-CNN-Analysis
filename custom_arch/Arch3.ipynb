{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%writefile archs.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Arch1: Modified VGG-style convolutional layers for 28x28x1 input, n_params = 648,974\n",
        "# Arch2: Modified ResNet model with BasicBlock (lesser blocks), n_params = 909,290.\n",
        "# Arch3: Modified DenseNet model with Bottleneck and Transition layers, n_params = 100k\n",
        "# Arch4: MobileNetV2-like model with DepthwiseSeparableConv layers, n_params = 149,566\n",
        "\n",
        "class Arch1(nn.Module):\n",
        "    def __init__(self, num_classes=62):\n",
        "        super(Arch1, self).__init__()\n",
        "\n",
        "        # Modified VGG-style convolutional layers for 28x28x1 input\n",
        "        self.features = nn.Sequential(\n",
        "            # Conv Layer 1 (Input: 28x28x1, Output: 28x28x64)\n",
        "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            # Conv Layer 2 (Output: 28x28x64)\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 14x14x64\n",
        "\n",
        "            # Conv Layer 3 (Output: 14x14x128)\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            # Conv Layer 4 (Output: 14x14x128)\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 7x7x128\n",
        "        )\n",
        "\n",
        "        # Fully connected layer without hidden layers\n",
        "        # Flatten the features from 7x7x128 to 6272 before feeding into the output layer\n",
        "        self.classifier = nn.Linear(7 * 7 * 128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = torch.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNetMod(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=62):\n",
        "        super(ResNetMod, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        # Adjust input conv layer for 28x28x1 input instead of 224x224x3\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "\n",
        "        # ResNet Layers\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        # The last two layers (residual blocks) are discarded as per your request.\n",
        "        # So, we stop here, no layer3, layer4\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # Adapted to small input size\n",
        "        self.fc = nn.Linear(128 * block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.avgpool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# Instantiate the modified ResNet model\n",
        "def Arch2(num_classes=62):\n",
        "    return ResNetMod(BasicBlock, [2, 2], num_classes)\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, in_channels, growth_rate):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
        "        self.conv1 = nn.Conv2d(in_channels, 4 * growth_rate, kernel_size=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(4 * growth_rate)\n",
        "        self.conv2 = nn.Conv2d(4 * growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(torch.relu(self.bn1(x)))\n",
        "        out = self.conv2(torch.relu(self.bn2(out)))\n",
        "        out = torch.cat([x, out], 1)\n",
        "        return out\n",
        "\n",
        "class Transition(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(Transition, self).__init__()\n",
        "        self.bn = nn.BatchNorm2d(in_channels)\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
        "        self.pool = nn.AvgPool2d(2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(torch.relu(self.bn(x)))\n",
        "        out = self.pool(out)\n",
        "        return out\n",
        "\n",
        "class DenseNetMod(nn.Module):\n",
        "    def __init__(self, num_classes=62, growth_rate=24, block_layers=[6, 6]):\n",
        "        super(DenseNetMod, self).__init__()\n",
        "        self.growth_rate = growth_rate\n",
        "        num_planes = 2 * growth_rate  # Starting number of planes\n",
        "\n",
        "        # Initial convolution layer\n",
        "        self.conv1 = nn.Conv2d(1, num_planes, kernel_size=3, padding=1, bias=False)\n",
        "\n",
        "        # Dense Block 1\n",
        "        self.block1 = self._make_dense_layers(Bottleneck, num_planes, block_layers[0])\n",
        "        num_planes += block_layers[0] * growth_rate\n",
        "        self.trans1 = Transition(num_planes, num_planes // 2)\n",
        "        num_planes = num_planes // 2\n",
        "\n",
        "        # Dense Block 2\n",
        "        self.block2 = self._make_dense_layers(Bottleneck, num_planes, block_layers[1])\n",
        "        num_planes += block_layers[1] * growth_rate\n",
        "        self.trans2 = Transition(num_planes, num_planes // 2)\n",
        "        num_planes = num_planes // 2\n",
        "\n",
        "        # Global average pooling and fully connected layer\n",
        "        self.bn = nn.BatchNorm2d(num_planes)\n",
        "        self.fc = nn.Linear(num_planes, num_classes)\n",
        "\n",
        "    def _make_dense_layers(self, block, in_channels, nblock):\n",
        "        layers = []\n",
        "        for i in range(nblock):\n",
        "            layers.append(block(in_channels, self.growth_rate))\n",
        "            in_channels += self.growth_rate\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.block1(out)\n",
        "        out = self.trans1(out)\n",
        "        out = self.block2(out)\n",
        "        out = self.trans2(out)\n",
        "        out = torch.relu(self.bn(out))\n",
        "        out = torch.nn.functional.adaptive_avg_pool2d(out, (1, 1))\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# Instantiate the modified ResNet model\n",
        "def Arch3(num_classes=62):\n",
        "    return DenseNetMod(num_classes=num_classes, growth_rate=12, block_layers=[4, 4])\n",
        "\n",
        "class DepthwiseSeparableConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(DepthwiseSeparableConv, self).__init__()\n",
        "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=stride, padding=1, groups=in_channels, bias=False)\n",
        "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.relu(self.bn1(self.depthwise(x)))\n",
        "        out = self.bn2(self.pointwise(out))\n",
        "        return out\n",
        "\n",
        "class Arch4(nn.Module):\n",
        "    def __init__(self, num_classes=62):\n",
        "        super(Arch4, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "\n",
        "        self.dw_conv1 = DepthwiseSeparableConv(32, 64)\n",
        "        self.dw_conv2 = DepthwiseSeparableConv(64, 128, stride=2)\n",
        "        self.dw_conv3 = DepthwiseSeparableConv(128, 128)\n",
        "        self.dw_conv4 = DepthwiseSeparableConv(128, 256, stride=2)\n",
        "        self.dw_conv5 = DepthwiseSeparableConv(256, 256)\n",
        "\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.dw_conv1(out)\n",
        "        out = self.dw_conv2(out)\n",
        "        out = self.dw_conv3(out)\n",
        "        out = self.dw_conv4(out)\n",
        "        out = self.dw_conv5(out)\n",
        "        out = self.global_avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    input_image = torch.rand(1, 1, 28, 28)\n",
        "    model = Arch1(num_classes=62)\n",
        "    output = model(input_image)\n",
        "    print(output.shape)\n",
        "    model = Arch2(num_classes=62)\n",
        "    output = model(input_image)\n",
        "    print(output.shape)\n",
        "    model = Arch3(num_classes=62)\n",
        "    output = model(input_image)\n",
        "    print(output.shape)\n",
        "    model = Arch4(num_classes=62)\n",
        "    output = model(input_image)\n",
        "    print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YLQoY2cRa8P",
        "outputId": "b6a29267-b528-4abe-fb14-2f055deb050d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing archs.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1CftXQwQUfz",
        "outputId": "c0d1143a-0353-41ec-e0d3-88c7d2fb05e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://biometrics.nist.gov/cs_links/EMNIST/gzip.zip to data/EMNIST/raw/gzip.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 561753746/561753746 [00:05<00:00, 102522519.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/EMNIST/raw/gzip.zip to data/EMNIST/raw\n",
            "Training set size: 593242\n",
            "Validation set size: 104690\n",
            "Test set size: 116323\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 580/580 [08:18<00:00,  1.16it/s, accuracy=0.64, loss=1.54]\n",
            "Validating: 100%|██████████| 103/103 [00:37<00:00,  2.78it/s, accuracy=0.808, val_loss=0.688]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Train Loss: 1.5414, Val Loss: 0.6878, Train Acc: 0.6404, Val Acc: 0.8077, Train Prec: 0.6304, Val Prec: 0.7713, Train F1: 0.5916, Val F1: 0.7723\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2: 100%|██████████| 580/580 [08:27<00:00,  1.14it/s, accuracy=0.838, loss=0.537]\n",
            "Validating: 100%|██████████| 103/103 [00:37<00:00,  2.76it/s, accuracy=0.845, val_loss=0.509]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10, Train Loss: 0.5365, Val Loss: 0.5091, Train Acc: 0.8379, Val Acc: 0.8446, Train Prec: 0.8123, Val Prec: 0.8222, Train F1: 0.8123, Val F1: 0.8218\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3: 100%|██████████| 580/580 [08:29<00:00,  1.14it/s, accuracy=0.852, loss=0.458]\n",
            "Validating: 100%|██████████| 103/103 [00:37<00:00,  2.74it/s, accuracy=0.837, val_loss=0.516]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10, Train Loss: 0.4584, Val Loss: 0.5165, Train Acc: 0.8523, Val Acc: 0.8370, Train Prec: 0.8319, Val Prec: 0.8191, Train F1: 0.8323, Val F1: 0.8170\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4: 100%|██████████| 580/580 [08:57<00:00,  1.08it/s, accuracy=0.858, loss=0.432]\n",
            "Validating: 100%|██████████| 103/103 [00:37<00:00,  2.73it/s, accuracy=0.854, val_loss=0.439]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10, Train Loss: 0.4318, Val Loss: 0.4393, Train Acc: 0.8578, Val Acc: 0.8539, Train Prec: 0.8393, Val Prec: 0.8424, Train F1: 0.8398, Val F1: 0.8424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5: 100%|██████████| 580/580 [08:23<00:00,  1.15it/s, accuracy=0.861, loss=0.417]\n",
            "Validating: 100%|██████████| 103/103 [00:37<00:00,  2.77it/s, accuracy=0.861, val_loss=0.426]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10, Train Loss: 0.4166, Val Loss: 0.4259, Train Acc: 0.8609, Val Acc: 0.8605, Train Prec: 0.8434, Val Prec: 0.8444, Train F1: 0.8439, Val F1: 0.8437\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 6: 100%|██████████| 580/580 [08:33<00:00,  1.13it/s, accuracy=0.864, loss=0.405]\n",
            "Validating: 100%|██████████| 103/103 [00:37<00:00,  2.76it/s, accuracy=0.86, val_loss=0.427]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10, Train Loss: 0.4052, Val Loss: 0.4268, Train Acc: 0.8636, Val Acc: 0.8600, Train Prec: 0.8469, Val Prec: 0.8477, Train F1: 0.8474, Val F1: 0.8437\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 7: 100%|██████████| 580/580 [08:23<00:00,  1.15it/s, accuracy=0.865, loss=0.399]\n",
            "Validating: 100%|██████████| 103/103 [00:37<00:00,  2.74it/s, accuracy=0.86, val_loss=0.422]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10, Train Loss: 0.3987, Val Loss: 0.4219, Train Acc: 0.8652, Val Acc: 0.8597, Train Prec: 0.8500, Val Prec: 0.8477, Train F1: 0.8495, Val F1: 0.8428\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 8: 100%|██████████| 580/580 [08:29<00:00,  1.14it/s, accuracy=0.867, loss=0.391]\n",
            "Validating: 100%|██████████| 103/103 [00:37<00:00,  2.78it/s, accuracy=0.865, val_loss=0.407]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10, Train Loss: 0.3913, Val Loss: 0.4073, Train Acc: 0.8674, Val Acc: 0.8654, Train Prec: 0.8526, Val Prec: 0.8511, Train F1: 0.8522, Val F1: 0.8510\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 9: 100%|██████████| 580/580 [08:20<00:00,  1.16it/s, accuracy=0.868, loss=0.387]\n",
            "Validating: 100%|██████████| 103/103 [00:36<00:00,  2.80it/s, accuracy=0.859, val_loss=0.425]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10, Train Loss: 0.3871, Val Loss: 0.4249, Train Acc: 0.8684, Val Acc: 0.8587, Train Prec: 0.8561, Val Prec: 0.8493, Train F1: 0.8535, Val F1: 0.8461\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 10: 100%|██████████| 580/580 [08:32<00:00,  1.13it/s, accuracy=0.869, loss=0.383]\n",
            "Validating: 100%|██████████| 103/103 [00:41<00:00,  2.48it/s, accuracy=0.868, val_loss=0.394]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10, Train Loss: 0.3828, Val Loss: 0.3944, Train Acc: 0.8695, Val Acc: 0.8680, Train Prec: 0.8562, Val Prec: 0.8561, Train F1: 0.8549, Val F1: 0.8506\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 114/114 [00:46<00:00,  2.43it/s, accuracy=0.867, loss=0.394]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.3941, Accuracy: 0.87%, Precision: 0.86, F1 Score: 0.85\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.models as models\n",
        "from torchvision.models import ResNet18_Weights\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import EMNIST\n",
        "from torch.utils.data import random_split\n",
        "from sklearn.metrics import precision_score, f1_score\n",
        "import torch.nn.functional as F\n",
        "from archs import *\n",
        "\n",
        "class ModifiedCrossEntropyLoss(nn.Module):\n",
        "    def __init__(self, penalty_weight=0.1):\n",
        "        super(ModifiedCrossEntropyLoss, self).__init__()\n",
        "        self.penalty_weight = penalty_weight\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        # Calculate probabilities using softmax\n",
        "        probs = F.softmax(inputs, dim=1)  # Get probabilities from raw logits\n",
        "\n",
        "        # Standard cross-entropy loss for the true class\n",
        "        loss_ce = torch.log(probs[range(targets.size(0)), targets] + 1e-12).mean()\n",
        "\n",
        "        # Calculate the penalty for all classes except the true class\n",
        "        penalty = self.penalty_weight * (torch.sum(torch.log(1 - probs + 1e-12), dim=1) -\n",
        "                                          torch.log(1 - probs[range(targets.size(0)), targets] + 1e-12))\n",
        "\n",
        "        # Final loss\n",
        "        total_loss = loss_ce + penalty.mean()\n",
        "        return -total_loss\n",
        "\n",
        "class ImageClassifier:\n",
        "    def __init__(self, network, optimizer, criterion, l2_lambda=0.01):\n",
        "        self.network = network\n",
        "        self.optimizer = optimizer\n",
        "        self.criterion = criterion\n",
        "        self.l2_lambda = l2_lambda\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.network.to(self.device)\n",
        "\n",
        "    def _regularize(self, network, l2_lambda):\n",
        "        # Compute L2 regularization\n",
        "        l2_reg = 0.0\n",
        "        for param in network.parameters():\n",
        "            l2_reg += torch.norm(param, 2)\n",
        "        return l2_lambda * l2_reg\n",
        "\n",
        "    def compute_loss(self, outputs, targets, l2_lambda=0.01, regularize = False):\n",
        "        # Compute the cross-entropy loss\n",
        "        ce_loss = self.criterion(outputs, targets)\n",
        "\n",
        "        if regularize:\n",
        "            # Compute regularization loss\n",
        "            l2_reg = self._regularize(self.network, l2_lambda)\n",
        "\n",
        "            return ce_loss + l2_reg\n",
        "\n",
        "        return ce_loss\n",
        "\n",
        "    def compute_metrics(self, preds, targets):\n",
        "        \"\"\"Helper function to compute accuracy, precision, and F1 score.\"\"\"\n",
        "        # Ensure preds are already in label form (if not already converted)\n",
        "        if preds.dim() > 1:  # Check if preds need reduction\n",
        "            preds = preds.argmax(dim=1)  # Get the predicted labels\n",
        "\n",
        "        preds = preds.cpu().numpy()  # Convert predictions to NumPy\n",
        "        targets = targets.cpu().numpy()  # Convert true labels to NumPy\n",
        "\n",
        "        # Compute accuracy\n",
        "        accuracy = (preds == targets).mean()\n",
        "\n",
        "        # Compute precision and F1 score using scikit-learn\n",
        "        precision = precision_score(targets, preds, average='weighted', zero_division=0)\n",
        "        f1 = f1_score(targets, preds, average='weighted')\n",
        "\n",
        "        return accuracy, precision, f1\n",
        "\n",
        "    def train(self, train_loader, val_loader, n_epochs=10, patience=3):\n",
        "        best_val_loss = float('inf')\n",
        "        current_patience = 0\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            # Train\n",
        "            self.network.train()\n",
        "            train_loss = 0.0\n",
        "            all_preds = []\n",
        "            all_targets = []\n",
        "\n",
        "            # Use tqdm for progress bar and set dynamic description\n",
        "            train_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Training Epoch {epoch + 1}')\n",
        "            for batch_idx, (data, target) in train_bar:\n",
        "                data, target = data.to(self.device), target.to(self.device)\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self.network(data)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = self.compute_loss(outputs, target)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                train_loss += loss.item()\n",
        "\n",
        "                # Gather predictions and true labels for accuracy/metrics calculation\n",
        "                preds = outputs.argmax(dim=1)\n",
        "                all_preds.append(preds)\n",
        "                all_targets.append(target)\n",
        "\n",
        "                # Update progress bar with loss and accuracy\n",
        "                current_accuracy, _, _ = self.compute_metrics(torch.cat(all_preds), torch.cat(all_targets))\n",
        "                train_bar.set_postfix(loss=train_loss / (batch_idx + 1), accuracy=current_accuracy)\n",
        "\n",
        "            # Calculate final metrics for training\n",
        "            all_preds = torch.cat(all_preds)\n",
        "            all_targets = torch.cat(all_targets)\n",
        "            train_accuracy, train_precision, train_f1 = self.compute_metrics(all_preds, all_targets)\n",
        "\n",
        "            # Validate\n",
        "            self.network.eval()\n",
        "            val_loss = 0.0\n",
        "            val_preds = []\n",
        "            val_targets = []\n",
        "\n",
        "            # Use tqdm for validation progress bar\n",
        "            val_bar = tqdm(val_loader, desc='Validating')\n",
        "            with torch.no_grad():\n",
        "                for data, target in val_bar:\n",
        "                    data, target = data.to(self.device), target.to(self.device)\n",
        "\n",
        "                    # Forward pass\n",
        "                    outputs = self.network(data)\n",
        "\n",
        "                    # Compute loss\n",
        "                    loss = self.compute_loss(outputs, target)\n",
        "                    val_loss += loss.item()\n",
        "\n",
        "                    # Gather predictions and true labels\n",
        "                    preds = outputs.argmax(dim=1)\n",
        "                    val_preds.append(preds)\n",
        "                    val_targets.append(target)\n",
        "\n",
        "                    # Update progress bar with validation loss and accuracy\n",
        "                    val_accuracy, _, _ = self.compute_metrics(torch.cat(val_preds), torch.cat(val_targets))\n",
        "                    val_bar.set_postfix(val_loss=val_loss / len(val_loader), accuracy=val_accuracy)\n",
        "\n",
        "            # Calculate final validation metrics\n",
        "            val_preds = torch.cat(val_preds)\n",
        "            val_targets = torch.cat(val_targets)\n",
        "            val_accuracy, val_precision, val_f1 = self.compute_metrics(val_preds, val_targets)\n",
        "\n",
        "            # Print epoch statistics\n",
        "            train_loss /= len(train_loader)\n",
        "            val_loss /= len(val_loader)\n",
        "            print(f'Epoch {epoch + 1}/{n_epochs}, '\n",
        "                  f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, '\n",
        "                  f'Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}, '\n",
        "                  f'Train Prec: {train_precision:.4f}, Val Prec: {val_precision:.4f}, '\n",
        "                  f'Train F1: {train_f1:.4f}, Val F1: {val_f1:.4f}')\n",
        "\n",
        "            # Check for early stopping\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                current_patience = 0\n",
        "            else:\n",
        "                current_patience += 1\n",
        "                if current_patience >= patience:\n",
        "                    print(f'Validation loss did not improve for {patience} epochs. Stopping training.')\n",
        "                    break\n",
        "\n",
        "    def test(self, test_loader):\n",
        "        self.network.eval()\n",
        "        test_loss = 0.0\n",
        "        correct = 0\n",
        "        all_preds = []\n",
        "        all_targets = []\n",
        "\n",
        "        # Use tqdm for test progress bar\n",
        "        test_bar = tqdm(test_loader, desc='Testing')\n",
        "        with torch.no_grad():\n",
        "            for data, target in test_bar:\n",
        "                data, target = data.to(self.device), target.to(self.device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self.network(data)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = self.compute_loss(outputs, target)\n",
        "                test_loss += loss.item()\n",
        "\n",
        "                # Gather predictions and true labels for accuracy/metrics calculation\n",
        "                preds = outputs.argmax(dim=1)\n",
        "                all_preds.append(preds)\n",
        "                all_targets.append(target)\n",
        "\n",
        "                # Update progress bar with test loss and accuracy\n",
        "                accuracy, _, _ = self.compute_metrics(torch.cat(all_preds), torch.cat(all_targets))\n",
        "                test_bar.set_postfix(loss=test_loss / len(test_loader), accuracy=accuracy)\n",
        "\n",
        "        # Calculate final test metrics\n",
        "        all_preds = torch.cat(all_preds)\n",
        "        all_targets = torch.cat(all_targets)\n",
        "        accuracy, precision, f1 = self.compute_metrics(all_preds, all_targets)\n",
        "\n",
        "        test_loss /= len(test_loader)\n",
        "        print(f'Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%, Precision: {precision:.2f}, F1 Score: {f1:.2f}')\n",
        "\n",
        "# Define transformation for the images\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),            # Convert to tensor (1 channel)\n",
        "    transforms.Normalize((0.5), (0.5))  # Normalize for RGB\n",
        "])\n",
        "\n",
        "# Download the EMNIST ByClass dataset\n",
        "emnist_dataset = EMNIST(root='data', split='byclass', train=True, download=True, transform=transform)\n",
        "test_dataset = EMNIST(root='data', split='byclass', train=False, download=True, transform=transform)\n",
        "\n",
        "# Define the sizes for the training and validation sets\n",
        "train_size = int(0.85 * len(emnist_dataset))  # 80% for training\n",
        "val_size = len(emnist_dataset) - train_size   # remaining 20% for validation\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_dataset, val_dataset = random_split(emnist_dataset, [train_size, val_size])\n",
        "\n",
        "print(f'Training set size: {len(train_dataset)}')\n",
        "print(f'Validation set size: {len(val_dataset)}')\n",
        "print(f'Test set size: {len(test_dataset)}')\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=1024)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1024)\n",
        "\n",
        "# Initialize the neural network, optimizer, and criterion\n",
        "model = Arch3(num_classes=62)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = ModifiedCrossEntropyLoss(penalty_weight=0.1)\n",
        "\n",
        "# Create an instance of ImageClassifier\n",
        "classifier = ImageClassifier(model, optimizer, criterion)\n",
        "\n",
        "# Train the classifier\n",
        "classifier.train(train_loader, val_loader, n_epochs=10, patience=3)\n",
        "\n",
        "# Test the classifier\n",
        "classifier.test(test_loader)\n",
        "\n",
        "torch.save(model.state_dict(), 'model3.pth')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.models as models\n",
        "from torchvision.models import ResNet18_Weights\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import EMNIST\n",
        "from torch.utils.data import random_split\n",
        "from sklearn.metrics import precision_score, f1_score\n",
        "import torch.nn.functional as F\n",
        "from archs import *\n",
        "\n",
        "class ModifiedCrossEntropyLoss(nn.Module):\n",
        "    def __init__(self, penalty_weight=0.1):\n",
        "        super(ModifiedCrossEntropyLoss, self).__init__()\n",
        "        self.penalty_weight = penalty_weight\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        # Calculate probabilities using softmax\n",
        "        probs = F.softmax(inputs, dim=1)  # Get probabilities from raw logits\n",
        "\n",
        "        # Standard cross-entropy loss for the true class\n",
        "        loss_ce = torch.log(probs[range(targets.size(0)), targets] + 1e-12).mean()\n",
        "\n",
        "        # Calculate the penalty for all classes except the true class\n",
        "        penalty = self.penalty_weight * (torch.sum(torch.log(1 - probs + 1e-12), dim=1) -\n",
        "                                          torch.log(1 - probs[range(targets.size(0)), targets] + 1e-12))\n",
        "\n",
        "        # Final loss\n",
        "        total_loss = loss_ce + penalty.mean()\n",
        "        return -total_loss\n",
        "\n",
        "class ImageClassifier:\n",
        "    def __init__(self, network, optimizer, criterion, l2_lambda=0.01):\n",
        "        self.network = network\n",
        "        self.optimizer = optimizer\n",
        "        self.criterion = criterion\n",
        "        self.l2_lambda = l2_lambda\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.network.to(self.device)\n",
        "\n",
        "    def _regularize(self, network, l2_lambda):\n",
        "        # Compute L2 regularization\n",
        "        l2_reg = 0.0\n",
        "        for param in network.parameters():\n",
        "            l2_reg += torch.norm(param, 2)\n",
        "        return l2_lambda * l2_reg\n",
        "\n",
        "    def compute_loss(self, outputs, targets, l2_lambda=0.01, regularize = False):\n",
        "        # Compute the cross-entropy loss\n",
        "        ce_loss = self.criterion(outputs, targets)\n",
        "\n",
        "        if regularize:\n",
        "            # Compute regularization loss\n",
        "            l2_reg = self._regularize(self.network, l2_lambda)\n",
        "\n",
        "            return ce_loss + l2_reg\n",
        "\n",
        "        return ce_loss\n",
        "\n",
        "    def compute_metrics(self, preds, targets):\n",
        "        \"\"\"Helper function to compute accuracy, precision, and F1 score.\"\"\"\n",
        "        # Ensure preds are already in label form (if not already converted)\n",
        "        if preds.dim() > 1:  # Check if preds need reduction\n",
        "            preds = preds.argmax(dim=1)  # Get the predicted labels\n",
        "\n",
        "        preds = preds.cpu().numpy()  # Convert predictions to NumPy\n",
        "        targets = targets.cpu().numpy()  # Convert true labels to NumPy\n",
        "\n",
        "        # Compute accuracy\n",
        "        accuracy = (preds == targets).mean()\n",
        "\n",
        "        # Compute precision and F1 score using scikit-learn\n",
        "        precision = precision_score(targets, preds, average='weighted', zero_division=0)\n",
        "        f1 = f1_score(targets, preds, average='weighted')\n",
        "\n",
        "        return accuracy, precision, f1\n",
        "\n",
        "    def train(self, train_loader, val_loader, n_epochs=10, patience=3):\n",
        "        best_val_loss = float('inf')\n",
        "        current_patience = 0\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            # Train\n",
        "            self.network.train()\n",
        "            train_loss = 0.0\n",
        "            all_preds = []\n",
        "            all_targets = []\n",
        "\n",
        "            # Use tqdm for progress bar and set dynamic description\n",
        "            train_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Training Epoch {epoch + 1}')\n",
        "            for batch_idx, (data, target) in train_bar:\n",
        "                data, target = data.to(self.device), target.to(self.device)\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self.network(data)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = self.compute_loss(outputs, target)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                train_loss += loss.item()\n",
        "\n",
        "                # Gather predictions and true labels for accuracy/metrics calculation\n",
        "                preds = outputs.argmax(dim=1)\n",
        "                all_preds.append(preds)\n",
        "                all_targets.append(target)\n",
        "\n",
        "                # Update progress bar with loss and accuracy\n",
        "                current_accuracy, _, _ = self.compute_metrics(torch.cat(all_preds), torch.cat(all_targets))\n",
        "                train_bar.set_postfix(loss=train_loss / (batch_idx + 1), accuracy=current_accuracy)\n",
        "\n",
        "            # Calculate final metrics for training\n",
        "            all_preds = torch.cat(all_preds)\n",
        "            all_targets = torch.cat(all_targets)\n",
        "            train_accuracy, train_precision, train_f1 = self.compute_metrics(all_preds, all_targets)\n",
        "\n",
        "            # Validate\n",
        "            self.network.eval()\n",
        "            val_loss = 0.0\n",
        "            val_preds = []\n",
        "            val_targets = []\n",
        "\n",
        "            # Use tqdm for validation progress bar\n",
        "            val_bar = tqdm(val_loader, desc='Validating')\n",
        "            with torch.no_grad():\n",
        "                for data, target in val_bar:\n",
        "                    data, target = data.to(self.device), target.to(self.device)\n",
        "\n",
        "                    # Forward pass\n",
        "                    outputs = self.network(data)\n",
        "\n",
        "                    # Compute loss\n",
        "                    loss = self.compute_loss(outputs, target)\n",
        "                    val_loss += loss.item()\n",
        "\n",
        "                    # Gather predictions and true labels\n",
        "                    preds = outputs.argmax(dim=1)\n",
        "                    val_preds.append(preds)\n",
        "                    val_targets.append(target)\n",
        "\n",
        "                    # Update progress bar with validation loss and accuracy\n",
        "                    val_accuracy, _, _ = self.compute_metrics(torch.cat(val_preds), torch.cat(val_targets))\n",
        "                    val_bar.set_postfix(val_loss=val_loss / len(val_loader), accuracy=val_accuracy)\n",
        "\n",
        "            # Calculate final validation metrics\n",
        "            val_preds = torch.cat(val_preds)\n",
        "            val_targets = torch.cat(val_targets)\n",
        "            val_accuracy, val_precision, val_f1 = self.compute_metrics(val_preds, val_targets)\n",
        "\n",
        "            # Print epoch statistics\n",
        "            train_loss /= len(train_loader)\n",
        "            val_loss /= len(val_loader)\n",
        "            print(f'Epoch {epoch + 1}/{n_epochs}, '\n",
        "                  f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, '\n",
        "                  f'Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}, '\n",
        "                  f'Train Prec: {train_precision:.4f}, Val Prec: {val_precision:.4f}, '\n",
        "                  f'Train F1: {train_f1:.4f}, Val F1: {val_f1:.4f}')\n",
        "\n",
        "            # Check for early stopping\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                current_patience = 0\n",
        "            else:\n",
        "                current_patience += 1\n",
        "                if current_patience >= patience:\n",
        "                    print(f'Validation loss did not improve for {patience} epochs. Stopping training.')\n",
        "                    break\n",
        "\n",
        "    def test(self, test_loader):\n",
        "        self.network.eval()\n",
        "        test_loss = 0.0\n",
        "        correct = 0\n",
        "        all_preds = []\n",
        "        all_targets = []\n",
        "\n",
        "        # Use tqdm for test progress bar\n",
        "        test_bar = tqdm(test_loader, desc='Testing')\n",
        "        with torch.no_grad():\n",
        "            for data, target in test_bar:\n",
        "                data, target = data.to(self.device), target.to(self.device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self.network(data)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = self.compute_loss(outputs, target)\n",
        "                test_loss += loss.item()\n",
        "\n",
        "                # Gather predictions and true labels for accuracy/metrics calculation\n",
        "                preds = outputs.argmax(dim=1)\n",
        "                all_preds.append(preds)\n",
        "                all_targets.append(target)\n",
        "\n",
        "                # Update progress bar with test loss and accuracy\n",
        "                accuracy, _, _ = self.compute_metrics(torch.cat(all_preds), torch.cat(all_targets))\n",
        "                test_bar.set_postfix(loss=test_loss / len(test_loader), accuracy=accuracy)\n",
        "\n",
        "        # Calculate final test metrics\n",
        "        all_preds = torch.cat(all_preds)\n",
        "        all_targets = torch.cat(all_targets)\n",
        "        accuracy, precision, f1 = self.compute_metrics(all_preds, all_targets)\n",
        "\n",
        "        test_loss /= len(test_loader)\n",
        "        print(f'Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%, Precision: {precision:.2f}, F1 Score: {f1:.2f}')\n",
        "\n",
        "# Define transformation for the images\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),            # Convert to tensor (1 channel)\n",
        "    transforms.Normalize((0.5), (0.5))  # Normalize for RGB\n",
        "])\n",
        "\n",
        "# Download the EMNIST ByClass dataset\n",
        "emnist_dataset = EMNIST(root='data', split='byclass', train=True, download=True, transform=transform)\n",
        "test_dataset = EMNIST(root='data', split='byclass', train=False, download=True, transform=transform)\n",
        "\n",
        "# Define the sizes for the training and validation sets\n",
        "train_size = int(0.85 * len(emnist_dataset))  # 80% for training\n",
        "val_size = len(emnist_dataset) - train_size   # remaining 20% for validation\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_dataset, val_dataset = random_split(emnist_dataset, [train_size, val_size])\n",
        "\n",
        "print(f'Training set size: {len(train_dataset)}')\n",
        "print(f'Validation set size: {len(val_dataset)}')\n",
        "print(f'Test set size: {len(test_dataset)}')\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=512)\n",
        "test_loader = DataLoader(test_dataset, batch_size=512)\n",
        "\n",
        "def Arch3_heavy(num_classes):\n",
        "    return DenseNetMod(num_classes=num_classes, growth_rate=24, block_layers=[6, 6])\n",
        "\n",
        "# Initialize the neural network, optimizer, and criterion\n",
        "model = Arch3_heavy(num_classes=62)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = ModifiedCrossEntropyLoss(penalty_weight=0.1)\n",
        "\n",
        "# Create an instance of ImageClassifier\n",
        "classifier = ImageClassifier(model, optimizer, criterion)\n",
        "\n",
        "# Train the classifier\n",
        "classifier.train(train_loader, val_loader, n_epochs=10, patience=3)\n",
        "\n",
        "# Test the classifier\n",
        "classifier.test(test_loader)\n",
        "\n",
        "torch.save(model.state_dict(), 'model3_2.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "R0Hgw3IPYtAN",
        "outputId": "ce5f412b-0b93-42a5-903f-2eece3034ca8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 593242\n",
            "Validation set size: 104690\n",
            "Test set size: 116323\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 1159/1159 [19:20<00:00,  1.00s/it, accuracy=0.803, loss=0.708]\n",
            "Validating: 100%|██████████| 205/205 [01:12<00:00,  2.81it/s, accuracy=0.846, val_loss=0.475]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Train Loss: 0.7078, Val Loss: 0.4749, Train Acc: 0.8029, Val Acc: 0.8458, Train Prec: 0.7837, Val Prec: 0.8293, Train F1: 0.7797, Val F1: 0.8249\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2:  86%|████████▌ | 997/1159 [16:12<02:38,  1.02it/s, accuracy=0.862, loss=0.409]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-7efac9c5adba>\u001b[0m in \u001b[0;36m<cell line: 249>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;31m# Train the classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;31m# Test the classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-7efac9c5adba>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_loader, val_loader, n_epochs, patience)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;31m# Update progress bar with loss and accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                 \u001b[0mcurrent_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m                 \u001b[0mtrain_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrent_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-7efac9c5adba>\u001b[0m in \u001b[0;36mcompute_metrics\u001b[0;34m(self, preds, targets)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# Compute precision and F1 score using scikit-learn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weighted'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_division\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     )\n\u001b[1;32m    212\u001b[0m                 ):\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.66666667\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m        \u001b[0;34m,\u001b[0m \u001b[0;36m0.66666667\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \"\"\"\n\u001b[0;32m-> 1293\u001b[0;31m     return fbeta_score(\n\u001b[0m\u001b[1;32m   1294\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mglobal_skip_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"skip_parameter_validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mfunc_sig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1483\u001b[0m     \"\"\"\n\u001b[1;32m   1484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1485\u001b[0;31m     _, _, f, _ = precision_recall_fscore_support(\n\u001b[0m\u001b[1;32m   1486\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mglobal_skip_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"skip_parameter_validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mfunc_sig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1791\u001b[0m     \u001b[0;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m     \u001b[0msamplewise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"samples\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1793\u001b[0;31m     MCM = multilabel_confusion_matrix(\n\u001b[0m\u001b[1;32m   1794\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1795\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mglobal_skip_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"skip_parameter_validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mfunc_sig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mmultilabel_confusion_matrix\u001b[0;34m(y_true, y_pred, sample_weight, labels, samplewise)\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m         \u001b[0msorted_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"y contains previously unseen labels: {str(diff)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msearchsorted\u001b[0;34m(a, v, side, sorter)\u001b[0m\n\u001b[1;32m   1398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1399\u001b[0m     \"\"\"\n\u001b[0;32m-> 1400\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'searchsorted'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mside\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mside\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msorter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7fgGpHxid6SK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}